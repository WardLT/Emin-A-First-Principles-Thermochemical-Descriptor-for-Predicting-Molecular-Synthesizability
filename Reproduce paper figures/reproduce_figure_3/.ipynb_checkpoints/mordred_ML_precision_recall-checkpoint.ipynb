{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a quick and simple ML model that predicts the synthesizability of materials using Mordred features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import *\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, plot_roc_curve\n",
    "from collections import defaultdict\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read in QM9 data with computed Mordred features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in QM9 data with computed Mordred features\n",
    "mordred_most_important_features = list(pd.read_csv('ML_mordred_feature_list/12_iteration_feature_33_features.csv')['Name'].to_numpy()[1:]) # start with index 1 since we want to skip over emin which is the first (most important) feature\n",
    "\n",
    "data = pd.read_csv(f'../../Computing Mordred Features for QM9/qm9_mordred.csv',usecols=mordred_most_important_features + ['Reported'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = data[mordred_most_important_features].to_numpy()\n",
    "training_labels = data['Reported'].to_numpy()\n",
    "\n",
    "# scramble ordering of features and labels to get a more homogeneously distributed dataset\n",
    "np.random.seed(123) # do not change this seed from 123\n",
    "scramble_indices = np.random.choice(range(len(training_labels)),size=len(training_labels),replace=False)\n",
    "\n",
    "training_features = training_features[scramble_indices]\n",
    "training_labels = training_labels[scramble_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:20<00:00,  2.06s/it]\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(max_depth=20,n_jobs=-1)\n",
    "\n",
    "splits = 10\n",
    "kf = KFold(n_splits = splits,shuffle=False) # DO NOT SHUFFLE\n",
    "kf_splits = kf.split(training_features,training_labels)\n",
    "training_predicted = []\n",
    "ground_truth = []\n",
    "\n",
    "for train_index, test_index in tqdm(kf_splits,total=splits):\n",
    "    \n",
    "    # define training and testing sets\n",
    "    X_train, X_test = training_features[train_index], training_features[test_index]\n",
    "    y_train, y_test = training_labels[train_index], training_labels[test_index]\n",
    "\n",
    "    # fit and predict\n",
    "    model.fit(X_train,y_train)\n",
    "    y_predicted = model.predict_proba(X_test)\n",
    "\n",
    "    training_predicted += [y_predicted]\n",
    "    ground_truth += [y_test]\n",
    "    \n",
    "all_predicted, all_ground_truth = np.concatenate(training_predicted,axis=0),np.concatenate(ground_truth,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracies(ground_truth, prediction_value, c):\n",
    "                   \n",
    "    ground_truth = 1*ground_truth # change bools to numbers\n",
    "    prediction = 1*(prediction_value >= c)\n",
    "\n",
    "    # count false positives\n",
    "    fp = np.sum((ground_truth == 0) & (prediction == 1))\n",
    "\n",
    "    # find false negatives\n",
    "    fn = np.sum((ground_truth == 1) & (prediction == 0))\n",
    "\n",
    "    # find true positives\n",
    "    tp = np.sum((ground_truth == 1) & (prediction == 1))\n",
    "\n",
    "    # find true negatives\n",
    "    tn = np.sum((ground_truth == 0) & (prediction == 0))\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    return precision, recall, tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process prediction probabilities [False, True] for reported/synthesizable\n",
    "\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "tp_list = []\n",
    "tn_list = []\n",
    "fp_list = []\n",
    "fn_list = []\n",
    "\n",
    "cutoffs = np.arange(0,1,0.01)\n",
    "\n",
    "for c in cutoffs:\n",
    "    \n",
    "    # for predictions on the validation data points\n",
    "    precision, recall, tp, tn, fp, fn = get_accuracies(all_ground_truth, all_predicted[:,1],c)\n",
    "    \n",
    "    precision_list += [precision]\n",
    "    recall_list += [recall]\n",
    "    tp_list += [tp]\n",
    "    tn_list += [tn]\n",
    "    fp_list += [fp]\n",
    "    fn_list += [fn]\n",
    "\n",
    "    \n",
    "\n",
    "precision_recalls = pd.DataFrame(data={'Cutoff': cutoffs,\n",
    "                                        'Precision': precision_list,\n",
    "                                        'Recall': recall_list,\n",
    "                                      'TP': tp_list,\n",
    "                                      'TN': tn_list,\n",
    "                                      'FP': fp_list,\n",
    "                                      'FN': fn_list})\n",
    "os.makedirs('precision_recalls',exist_ok=True)\n",
    "precision_recalls.to_csv(f'precision_recalls/mordred_ML_precision_recalls.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d97570bacec7fd5be4af27e1255edf48f45d2285fbfe7cdaecfe9946c7652b1c"
  },
  "kernelspec": {
   "display_name": "my-rdkit-env",
   "language": "python",
   "name": "my-rdkit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
